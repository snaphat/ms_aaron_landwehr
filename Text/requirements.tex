\label{chap:requirements}
In this chapter, we will discuss the underlying hardware requirements in order to implement a self-adaptive system. Many of these will be integral toward enabling a self-adaptive system software and others will simply improve or make its job easier in adapting. Primarily this section serves as a list of features that we believe are important for adaptation in any exascale architecture. Much of the discussion found within parallels my own prior work~\cite{Landwehr2014}. Before we discuss these requirements in detail, we will first discuss the types of adaptation that we target followed by a discussion of the benefits of a tailored Performance Monitoring Unit (PMU).

\section{Types of Adaptation}
%{
    When it comes to a self-aware system, there are a number of types of adaptation that need to occur. These can be categorized broadly into three distinct types of adaptation: adapting for energy or power consumption, adapting for performance, and adapting for resiliency.

    Energy adaptation covers any decisions that take into account energy and power in some manner. This can include such things as monitoring the energy usage of tasks directly or indirectly, as well as, maintaining a given energy or power envelope. This can also include any decisions that increase performance with a primary focus on reducing energy. An example of energy adaptation would be to identify whether a task is utilizing specific functional units (FUs) and to clock or power gate them if they are not in use. Performance adaption covers any decision that has a primary goal of increasing performance. Such decisions might take into account system and network utilization as well as characterize tasks by the types of operations they are doing. For example, this could be to identify whether a task is CPU bound, I/O bound, etc. Resiliency adaptation covers any sort of detection and/or prevention of faults as well as the handling of task recovery in the event of failure. Task recovery can entail a multitude of different features such as task migration and memory redundancy.

    While we will discuss all three types of adaption, the primary purpose of this thesis is to focus on a path forward for demonstrating power and temperature adaptation in our Target Exascale Architecture. It is important to note that there is necessary coupling between all three categories of adaptation. An example of overlap between performance and energy adaptation is intelligently scheduling tasks and movement of data to increase performance while at the same time reducing energy consumption. An example of overlap between all three types of adaptation is building in hardware support for the detection of data corruption. Without hardware support, a resilient runtime would be forced to duplicate work and provide checksumming which increases energy and decrease performance at the same time.
%}

\section{Role of the PMU in Exascale Systems}
%{
    Before moving onto the detailed requirements in the subsequent sections, we will discuss the Performance Monitoring Unit (PMU) as an important mechanism toward enabling adaptation. From an energy perspective, direct energy counters may implemented such as seen in the Running Average Power Limit (RAPL) counters of Intel architectures. Another mechanism could be to combine instruction counters with instruction energy cost metrics in order to indirectly monitor energy. From the perspective of performance monitoring, the PMU can directly give many different instruction count metrics that are useful in characterizing performance. From a resiliency perspective, counts of correctable errors can be used to aid in pro-actively monitoring for potential issues and as a mechanism to determine whether the system software should be cautious with the work it is scheduling. A plethora of counters and the ability to run them concurrently will greatly aid in information gathering for a self-adaptive system. As such, it is our belief that the PMU will serve as the primary means of information gathering for both performance and energy adaptation and will be one of the most important mechanisms of a self-adaptive system.
%}

\section{Energy Requirements}
%{
    For any large-scale computer system (including current petascale ones), the primary goal in self-adaptation is to minimize energy consumption. As discussed previously, the PMU will be integral to this goal. 

    At the most basic level, the PMU provides various performance related metrics. This may include the accumulated energy expenditure, as well as, the counts of various different instruction types such as local/remote reads, writes, arithmetic logic unit (ALU) operations, floating-point unit (FPU) operations, direct memory access (DMA) operations, etc. These counts are useful directly for determining the workload characteristics and optimality of running tasks (and of higher level components in the system). For example, if the runtime system is able to determine that a task is spending the majority of its time idling while waiting for remote memory through the usage of some combination of remote read and DMA operations, it could clock gate the processor running the task, while the data of the task is moved to a more localized memory. For another example, through the count of FPU operations, the runtime system could determine that only integer calculations are performed on a given eXecution Engine (XE), and thus decide to power-gate its FPU.

    The PMU can also be used indirectly to estimate energy usage. This is possible if the energy cost of various instructions and components in the system are known or estimable, and an energy model is developed. Essentially, the costs of instructions could be combined with the counts read from the PMU to form a picture about the overall energy usage. This information would then be used in conjunction with specified power budgets to determine if actions need to be taken in order to meet goals.
    
    %Although, it is worth noting that there are number of drawbacks with this solution: (1) software models can be inaccurate, (2) the energy cost of calculating the power consumption could be costly, (3) hardware support could give us additional features that this method is not capable of.

    We expect exascale architectures to be capable of adjusting the state of functional units (FUs) directly without which they will be unable to cope with power expenditure budgets or unable to operate at all due to thermal levels. Some may even be capable of adjusting state at the functional unit block (FUB) granularity. To clarify, this is at a finer granularity than FUs, meaning that individual sub-unit pipelines can be clock gated or power gated. At an individual core level, it is useful to be able to adjust both the clock rates, and to power down unused cores in order to save energy. An example of the former is simply adjusting the clock rates to meet a specified power budget. Another example outside of the realm of energy only adaptation is that some cores may not be able to run at a full clock-rate due to physical defects or the current thermal characteristics of the system. It may be advantageous in this case to use the cores at a lower clock-rate than to simply put them into a power-gated state. As a final motivating example, a self-adaptive runtime could identify a case where the rate of data being streamed into a block is lower than the XE using the data for computation. In this case, the computation XEs would need to idle waiting for the data. The control engine could identify this and then individually slow the clock rate of the XEs to match that of the rate at which the data is being streamed in.
    
    We believe it is advantageous to adjust at an even finer-granularity than simply the core. One interesting motivation would be for task kernel hinting. Given a compiler with the capability to identify the types of instructions used by a task kernel and given mechanism for the runtime to hook into this information, it could identify explicitly which units would not be used by a given task and simply power gate them. Furthermore, the same strategy could be applied at an even finer-granularity to turn off individual pipelines within FUs.

    %One important aspect of self-adaption will be the estimation of energy/power consumption for specific tasks in order in order to meet specified budgets. As mentioned previously, ideally the hardware would support monitoring the energy consumption at the XE level. However, were this to not happen a fallback requirement would be to be able to monitor the temperature of individual XEs. While these do not directly give you energy consumption, they can aid in determining which XEs are using larger amounts of energy within a given block.

    %One  alternative mechanism for hardware supported energy monitoring is the Running Average Power Limit (RAPL) interfaces found in the Sandybridge, Ivybridge, and Haswell architectures~\cite{IntelManual}. These directly provide energy/power information, power limiting, and policy controls in the form of machine specific registers (MSRs). Using these interfaces, it is possible to retrieve both power (watts) and energy (joules) directly as well as specify a goal in terms of watts over a given time interval. Finally, these interfaces are able to give information about the amount of time that a component has been throttled in order to meet a specified goal.

    %FIXME: Discuss Thermal monitoring here in terms of energy

    %[Any other kinds of energy detection/monitoring?]
    %
%}

\section{Performance Requirements}
%{
    In exascale systems, the system software will be responsible for task scheduling and resource allocation. Thus it needs to be able to monitor performance in order to achieve adaptation. There are various types of performance metrics that will be important. These can range from different types of resource utilization (network, CPU, memory, etc.) to workload distribution, etc. Characterizing sections of the system will require monitoring to be relatively fine-grained. We believe that the ideal granularity is at the task level. We discussed previously some examples of the dual energy/performance adaptations that can be yielded from PMU metrics. As we mentioned, the key importance here is that the PMU is useful in determining the workload characteristics and optimality of running tasks within the system.

    %FIXME: reword the above. Separate into two paragraphs.
    %Motivate the importance of task level granularity

    Using the PMU events described, performance within the runtime can be evaluated. To further motivate the usefulness of knowing this information, let us consider another example. By knowing the frequency and types of memory counts, the system software can determine network utilization and whether the communication is relatively localized. This information is useful for determining how optimal the current task scheduling is in terms of performance and energy efficiency. If for instance, the system software can determine that groups of tasks are communicating frequently but are not localized to the same block, it could migrate the tasks to one block in order to localize the network traffic.

    %FIXME: Consider another example or tighten this one up? SZ says the latter
    %Check the energy section when rewording this - because it has examples... can we combine these?
%}

\section{Resiliency Requirements}
%{
    Though the focus of this thesis is not on adaptive fault tolerance, we discuss resiliency in terms of hardware/software requirements for completeness sake because as mentioned previously, the goals of power, performance, and resiliency are not independent or mutually exclusive in an exascale architectures. As such, fault tolerance is necessarily an important aspect of self-adaptive exascale systems. Without proper hardware support, the software will be unable to cope with failures or to meet goals. Furthermore, the system software would necessarily be burdened with the detection and prevention of faults through costly primitive means. This could potentially entail such things as the duplication of tasks and verifying the results of all task computations within the system. In short, lack of proper hardware support for resiliency will significantly affect other aspects of self-adaptation.

    It is our belief that the hardware must be able to detect faults within the components of the system. The primary motivation for this is to minimize runtime scope and energy costs. A system software burdened with the aforementioned details will be extensive and inefficient. This leads not only to a high cost in software support, but also a reduction in energy efficiency and performance. For example, task duplication could force the same task to be re-run three times simply to determine which set of components is faulty.

    However, the hardware needs not only to detect and/or correct faults but also a means to deliver information about the failure to the system software. As such, it is essential for an introspective system software to know which FUs have failed in order to reschedule any tasks that require or depend on the failed hardware resources.

    Even with proper hardware detection for faults, the software system is burdened by the need to schedule tasks efficiently in a non-ideal environment. From a self-adaptive standpoint, one of the primary tasks will be to achieve a known working state. On current generation systems, the primary means by which this is achieved is through extensive quality control tests. In the software stack of an extreme scale environment, a map of known permanently bad components from quality control testing will be essential for the system-software to learn and adapt as quickly and efficiently as possible. This will yield a known good state on the start up of the system without spending large amounts of energy determining which parts of the system are bad and/or scheduling tasks to faulty components. This map would need to contain two types of information: all components that fail under any circumstances, and components that fail under certain known operating conditions. For example, the latter could contain Vdd requirements, thermal requirements, limits to the number of XEs that can operate together, etc.

    %FIXME: Discuss a software based map for intermittent faults to supplement the above map

    Another aspect of fault tolerance is proactive preventive measures that can be taken in both hardware and software. For instance, if the system-software can identify components with a high probability of failure, it can avoid scheduling critical tasks to those components. The envisioned form of support is through the dual usage of error correction and the tabulation of errors exposed through performance monitoring units. Given such details, the system-software could keep track of the errors and use some form of built in risk assessment when scheduling and allocating resources.

    %FIXME:            \textsc{\textbf{SZ: Try to be more specific. What would be a critical type of codelet/EDT/piece of code?}}
    %FIXME:
    %[One important ability could be to turn off any hardware remapping feature for failed components. Ideally the software system should know about all failed components to aid in better self-adaptation in the event that remapped components affect performance in some manner.]
    %[Figure out how to better discuss the above]
    %[Discuss Thermal monitoring here in terms of resiliency - SZ says this is important]
%}
