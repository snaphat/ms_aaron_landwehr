\label{chap:related_work}
This chapter discusses prior related work in self-adaptation and self-awareness to give the reader a brief understanding of the many different aspects of adaptation. Approaches range from designing adaptive hardware to cross-layer techniques that focus on using hardware and software together to build adaptive systems or applications. Many approaches focus on specific applications or subclasses of application and as such do not consider any notion of system level adaptation which is of an integral importance within exascale systems. Other approaches focus on providing some type adaptation for specific components within a system and could be incorporated into exascale hardware or even exposed to system software. Research into system level adaptation is generally in the form operating systems or management of resources. Newer bodies of work attempt to treat adaptation more holistically, and provide solutions that span multiple layers of the system from hardware to the high-level software.

For the purposes of this discussion, we group different approaches to adaptation into four categories: Application Centric Adaptation, Component Centric Adaptation, System Centric Adaptation, and Cross-layer Adaptation. {\it Application Centric Adaptation} focuses on adaptation within specific applications using either hardware or software techniques. {\it Component Centric Adaptation} focuses on monitoring or adaptation within a particular component or resource of a system. {\it System Centric Adaptation} focuses on adaptation at the system level. {\it Cross-layer Adaptation} focuses on adaptation across multiple layers of the system software stack. Following this, we discuss the aspect of predictive system modeling. Finally, we discuss state of the art self-adaptive frameworks and the features they exhibit and the aspects of exascale adaptation that make it fundamentally different from prior research.

\section{Application Centric Adaptation}
%{
    Application centric approaches focus on adaption for specific applications or a subclass of applications within a given domain. Because this type of adaptation is largely not generalizable outside of specific applications; and thus, incompatible with our goal of a general self-aware system, we mention these only in passing for completeness sake.
    
    Quality of service (QoS) is one large area of active research~\cite{AbdelzaherShin1998, CardelliniEtAl2012, IvanovicEtAl2010, LuEtAl2006, ZhengEtAl2010} due to the real time requirements and the ever changing field of computing resources. Many newer works focus on cloud computing in particular~\cite{KumarEtAl2013, WangEtAl2013}. Other works focus on changing application specific algorithm policy~\cite{ThomasEtAl2005}.
    
    Some approaches are more akin to toolkits designed for application programmers to use to enable adaptation within their software~\cite{GoelEtAl1998, GoelEtAl1999}. Many of these approaches listed incorporate some form of classical control theory because of various provable guarantees such as stability and linearity, etc.; however, in more recent years, there has also been a shift toward heuristic based tuning and machine learning techniques~\cite{MaggioEtAl2011} because, though not providing the provable guarantees of classical control theory, these tend to provide more robust adaptivity in practice with real applications.
%}
\section{Component Centric Adaptation}
%{
    Component centric approaches are a form of adaptation that focuses on monitoring and adapting a particular components or resources of a system. These can include such facets as networking, memory, and cache, among other things. Many of these works focus on introducing dynamic reconfigurability into the hardware or at the software level. There is much work to be found along these lines but for brevities sake we focus on only a few newer works.
    
    Some work focuses on introducing adaptive reconfigurability into memory organization. One such work~\cite{SaM_2013} proposes a self-aware memory (SaM) which includes a self-optimization process. The memory itself is partitioned into self-managing components and dynamic memory allocation is done using a client-server style technique to reduce bottlenecks. In order to optimize memory allocation, each memory component has knowledge of allocated blocks, access rate, and ownership, as well as, limited knowledge of the state of neighboring memory components. An extended memory management unit (MMU) is responsible for memory requests and uses acquired knowledge to to optimize memory allocations. Another work~\cite{IpekEtAl2008} proposes a self-optimizing memory controller that operates using the principles of reinforcement learning (RL). It works by formulating the scheduling of data reads and writes as a reward based structure. If a command leads to data bus utilization, this is marked as a reward of one; otherwise, zero. Using accrued data in conjunction with the controller's state (number of reads, writes, and misses, etc.), the controller can learn to optimize various aspects of its operation, such as, balancing reads and writes, detect states that lead to low levels of requests and avoid those in advanced by prioritizing load misses, amortize write-to-read delays, etc.
    
    Other work focuses on introducing hardware reconfigurability into the caching aspects of memory. One such work~\cite{Balasubramonian2000,BalasubramonianEtAl2000} proposes incorporating dynamic reconfiguration into the caching levels of memory hierarchies. In essence, the hardware is capable of detecting phase changes in the access pattern of applications to react to hit and miss intolerance by adjusting the hierarchy from a three-level cache to a two-level cache and vice versa. Another work~\cite{WangMishra2009} proposes dynamically adjusting the associativity and line sizes using similar techniques. Still other works~\cite{KimEtAl2002} focus on the non-uniform cache access (NUCA) times associated with large multi-megabyte caches and propose logical policies to allow data to migrate adaptively to memory banks with less cycle access times for given processors depending on the access pattern.
    
    There are a number of works that focus on introducing adaptive reconfigurability into networks or network interfaces. One such work~\cite{NoC_2013, NoC_2014} develops a network-on-chip (NoC) with dynamically configurable memory buffers (FIFO depth) and a dynamically configurable time division multiple access (TDMA) scheduler which can adapt the number of time slots for different communications according to measured bandwidth. Hardware mechanisms are incorporated to expand and shrink FIFOs depending on utilization. Additionally, genetic algorithms are used to optimize TDMA table access. There has been additional work done on tackling network adaptation at the software level. One such work~\cite{Gelenbe2013} focuses on retaining past QoS performance information for given hops along destination from source to enable the source to make better decisions about the path to send packets along in the future.
%}

\section{System Centric Adaptation}
%{
    \label{sec:SystemCentric}
    System centric approaches focus on adaptation at the system level. These are software level approaches that tend to use existing hardware and the operating system (OS) for adaption. Historically the role of resource management has been given to the operating system. In the case of highly parallel systems, we can divide these approaches into several categories: full OSes, lightweight kernels, micro kernels, and high-level runtime systems.

    \subsection{Full OSes}
    %{
        High-performance systems which use so-called Full OSes take advantage of off-the-shelf systems, and tune them to reduce system noise. Such approaches are often embodied in cluster-like environments~\cite{SterlingEtAl95}. Even on dedicated supercomputers, these approaches have been followed, as they provide a programming environment which allows for maximal flexibility. However, such systems are in general ill-prepared for the requirements of future extreme-scale/exascale high-performance environments: their control over the power envelope is only at a very coarse-granularity; resilience is left to third party systems, and is not considered as part of the whole; they are oblivious of the needs of the application they host; etc. Finally, full OSes leave very little room for specialization.
    %}

    \subsection{Light-Weight Kernels}
    %{
        Another approach is to rely on so-called light-weight kernels or LWKs~\cite{GiampapaEtAl10, BallesterosEtAl12}. LWKs offer many advantages over full OSes: They are usually written from scratch, and only re-implement features needed for an HPC environment. The source code being much smaller, means bugs are easier to track and fix. Finally, being specialized kernels, they usually emit very little noise when running an application on top of LWKs. They also do expose significant limitations: they very often require the user to learn a new API to communicate with the system; if a feature usually provided by full OSes is missing from the LWK, the user's application may not be portable to the system. In general, the application programmer does require features found in traditional OSes. Some LWKs do forward calls to ``missing'' features to a  ``heavier'' kernel however. %\textbf{[TODO: \\cite]}
    %}

    \subsection{Micro-Kernels}
    %{
        Micro-kernels strip down OSes to the bare minimum (i.e. address space management, process/thread management, inter-process communication). These so called kernels run in privileged mode (e.g. ring0 on x86 architectures), while providing satellite features which enrich the overall system, but in an unprivileged mode (for example, a file system driver). Micro-kernel OSes have shown they could be robust and thus fulfill the resiliency and maybe even the power and energy requirements (as only the required services are running). However, for many years, performance was lacking, due to the message-driven orientation of most implementations. 

        New approaches have tried to revive micro-kernels, as well as, remove the layers that historically introduced overhead~\cite{AppavooEtAl05,KriegerEtAl06,NightingaleEtAl09,SachaEtAl12}. Indeed, message-driven communication in micro-kernels tend to suit multi- and many-core systems very well. However, there is no approach that tries to provide a holistic view of performance, power, and resiliency for different granularities. However the latest efforts around micro-kernels have evolved toward a more library-oriented approach for operating systems~\cite{AmmonsEtAl07}. Such approaches tend to have goals that are closer to our own.
    %}

    \subsection{High-level Runtime Systems}
    %{
        High-level runtime systems typically implement some form of resource management on top of an existing OS. Typically they intend to provide some type of policy management that doesn't exist in the underlying OS. Some focus on providing QoS of system resource~\cite{LiNahrstedt2001, LiEtAl2006, Sharifi2011} or provide resource management with dynamic policies~\cite{ZhangEtAl2002}. Others are in the form of languages or frameworks that provide mechanism for an application to adapt~\cite{ChangKaramcheti2000, HollingsworthEtAl1998, RiblerEtAl1998, SironiEtAl2012}.
    %}
%}

\section{Cross-layer Adaptation}
%{
    Cross-layer approaches involve implementing introspective capabilities across multiple levels of the system software stack. These differ from the system level approaches discussed in section~\ref{sec:SystemCentric} in that they are designed specifically to expose capabilities to differing levels of the software stack; whereas, system centric approaches tend to keep most control at the operating system level. For example, these approaches may place monitoring capabilities at the OS kernel level and adaptive capabilities at the application level. These types of approaches served as inspiration for the work contained within this thesis.
    
    The SElf-awareE Computing (SEEC) model~\cite{Hoffmann2010,Hoffmann2012,Hoffman2013} by Hank Hoffman in many ways is the father of self-aware computing, as well as, father of cross-layer approaches to adaptation. It incorporates an observe-decide-act feedback loop with, at its core the notion of expressing goals as application heartbeats. In essence, using a HeartBeat API, an application registers heartbeats at some specified interval as well as a target heart rate to reach. Additionally, a precision goal can be registered (e.g. maximize performance at some user defined peak-signal-to-noise ratio - PSNR). In the background, SEEC observes the heart rate and the controller calculates the adjustments needed at the next time step. Offline, a series of actuators, possible states, and the costs/benefits of those states are defined. The cost/benefits associated with each actuator are used in the decision making process to determine the best course of action in the next time step. SEEC then applies actions by adjusting actuators to the appropriate state. Actuators can either be system level (e.g. core frequency) or application specific (e.g. which video codec to use). Additionally, reinforcement learning is used to adjust the costs/benefits of each actuator, thus enabling the system to eventually converge to meet the demands of the target heart rate. Video decoding is an often seen example application used with SEEC. In this particular case, frames-per-second (FPS) can easily be expressed as a target heart rate. After each frame is rendered the application registers a heart beat and SEEC keeps track of the heart rate (FPS). If the current heart rate is either lower or higher than than the target heart rate then SEEC will adjust a number of different actuators until the heart rate is met. In this particular case, various decoding parameters can be adjusted on the fly as well as system specific actuators. Because the actuators affect the quality of rendered images, a PSNR goal can be used to ensure that the output quality does not become too low. Similarly, power goals can be specified to allow for the application to reach a target heart rate while minimizing power usage. The generalized controller design and cross-layer nature of SEEC has shown promising results when used in conjunction with specialized hardware features. One such work combines SEEC with a processor that incorporates energy monitoring circuits~\cite{SinangilEtAl2014} to augment SEEC's energy adaptation capabilities.
    
    Another work, CoAdapt~\cite{Hoffmann2014}, also by Hoffman, seeks to alleviate some of the shortcomings of the prior work. While SEEC could only provide guarantees for a single dimensional goal in terms of power or performance, or accuracy; CoAdapt, can provide guarantees for two out of the three dimensions while also optimizing the third. CoAdapt segments goal dimensions such that one is the lead and the other is the subordinate. The overall configuration of the lead dimension is used to predict how the primary goal will affect the subordinate dimension and the system uses this information to dynamically adapt and ensure the latter dimension doesn't oscillate and instead converges to its goal.
    
    Some approaches focus on runtime level adaptation using information from lower levels of the software stack. A work, by Gioiosa~\cite{GioiosaEtAl2014}, applies classical control theoretic techniques to implement adaptive behavior across the software stack using a self-feedback control loop. At the kernel level, a kernel module based monitor observes current system values and compares those to reference values in order to apply self-correction. A controller, implemented at the runtime level (e.g. OpenMP, etc.), communicates with the monitor to make adjustments, such as, adjusting the number of process threads. A well defined communication interface is used to facilitate the sending and receiving of information and commands between the components. The primary focus of the work is on decoupling the low-level details of the system (from a monitoring sense) from high-level programming models or runtimes which want to incorporate adaptive decision making.
%}

\section{Modeling}
%{
    A number of works focus primarily on the aspect of predictive modeling of system behavior. Though these works do not necessarily implement a cross-layer adaptive approach, they are an integral piece in an adaptive solution that could be used in any layer of a software stack from application level to runtime level. For this reason, we discuss these works here. It is worth noting that these types of work do not conflict with work found in this thesis, and indeed, we expect modeling to be an important aspect of the predictive capabilities of future exascale systems.
    
    One work develops a power model based on PMU information and extends linear and neural network models to account for external temperatures to build an adaptive predictive power modeling solution for existing hardware~\cite{AhmadEtAl2015}. The model separates events into three categories: (1) local within a core, (2) events occurring within shared resources, and (3) events available at the OS level. Algorithmic models are used to adaptively choose relevant events for modeling while discarding irrelevant events for performance overhead reasons. The novelty of the approach is that the interactions of events is considered during the selection process. The selection process uses a data mining inspired sub-space method to greedily search for the best events using Correlation Feature Selection~\cite{HallHolmes2003}. For power modeling, a linear approximation of traditional RC circuits is used.
%}

\section{Discussion}
%{
    \label{sec:related_work_discussion}
    This section is devoted to discussion of the important aspects toward developing a truly self-aware exascale system. Let us begin by discussing the numerous challenges of exascale and then following this by a discussion of the general limitations in current bodies of research. Some of the important challenges of exascale include the management of power and energy~\cite{Dally2011}, as well as, performance across hundreds to thousands of cores. Furthermore, these seemingly conflicting goals cannot be considered in isolation. A coordination between system resources, components, and applications will be integral at the exascale level to coordinate and adapt.

    In terms of exascale applicability, application level approaches to adaptation focus far too narrow in scope on specific applications and lack a holistic view. These tend to lack an appreciation for the system level challenges discussed above, and in many cases could directly conflict with or hinder cross-layer adaptive goals, such as, minimizing energy expenditure. That is not to say that application goals should not be considered in an exascale system. In fact, applications will need to become first class citizens in the sense that their goals will need to be accounted for by a self-adapting system. However, application goals should be expressed in the form of hints to system level software given that such software will have better and more complete information to make scheduling and other decisions upon.
    
    For exascale adaptivity, component level adaptation does not necessarily hinder a cross-layer approach to adaptation. Indeed, if the right interfaces were exposed within the hardware, it could very well aid a cross-layer approach to adaptation. And software level component adaptation (such as software QoS) could be incorporated directly into cross-layer approaches. For the most part, these types of approaches are neither here nor there with respect to the work discussed in this thesis; however, we do take inspiration from these in terms of exposing hardware features or creating software features that allow cross-layer adaptation of specific hardware components.
    
    Research into system level adaptation, as discussed previously, is generally in the form of operating systems or in the management of resources. However, these approaches lack fine-grained control over hardware components due to limitations in hardware. Exascale architectures will need to adjust the state of components at a very fine granularity in order to conserve energy and to meet power envelopes. This is one important area where exascale research breaks from the current state of the art in that a co-design of hardware and software will yield a system that is capable of adapting in ways not possible in current generation systems. As such, outside of limited research, system software design has not generally attempted to conquer the challenges of exascale. Moreover, there is a large change in scope both in terms of hardware heterogeneity and the vast expansion in the number of components that will need to be taken into account for adaptation. As previously mentioned, there is also an expansion in the goals and types of adaptation that need to occur in exascale systems.
    
    The current trend in adaptive computing has been to focus on energy adaptation in some form~\cite{BaekEtAl2010, SorberEtAl2007} as this is integral for low power domains and now increasingly for exascale architectures. SEEC, one of the seminal works in self-aware computing,~\cite{Hoffman2013} focused on both energy and performance adaptation using a notion of ``application heartbeats.'' These allowed for an application to communicate goals to a system as well as progress toward those goals. The results showed that various applications could be instrumented with heart beats and capable of adapting to meet both energy and performance goals. However, SEEC relied heavily on application instrumentation and thus is arguably limited to applications that fit a certain paradigm. Additionally, it supported only application goals; as opposed to more more general system level goals. It also relied on the programmer to understand and provide a notion of progress toward a goal; which may not be possible in all applications. Finally, it lacked support for handling conflicting goals or more generally multi-variable problems. CoAdapt~\cite{Hoffmann2014} sought to eliminate some of these shortcomings by providing limited support for adapting to multiple conflicting goals; however, many of the other challenges and limitations mentioned above were not addressed. Other cross-layer approaches while focusing on exposing hardware monitoring to runtime level software tend to focus on existing hardware and thus tend to be narrow in scope in terms of applicability to future scale systems.
%}
